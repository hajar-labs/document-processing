import re
import spacy
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import json
from dataclasses import dataclass, asdict
from collections import Counter
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentAnalysis:
    """Classe pour structurer les r√©sultats d'analyse"""
    type_document: str
    confidence_score: float
    language: str  # 'fr', 'ar', 'mixed'
    personnes: List[str]
    lieux: List[str] 
    dates: List[str]
    secteur: str
    mots_cles: List[str]
    organisations: List[str]
    numeros_reference: List[str]
    
class BilingualDocumentAnalyzer:
    """Analyseur intelligent de documents minist√©riels fran√ßais et arabes"""
    
    def __init__(self):
        # Chargement des mod√®les spaCy
        try:
            self.nlp_fr = spacy.load("fr_core_news_sm")
        except OSError:
            logger.warning("Mod√®le fran√ßais non trouv√©. Installez avec: python -m spacy download fr_core_news_sm")
            self.nlp_fr = None
            
        # Dictionnaires de classification - Fran√ßais
        self.types_documents_fr = {
            "cahier_charges": {
                "keywords": [
                    "cahier des charges", "appel d'offres", "march√© public", "consultation",
                    "soumissionnaire", "candidat", "prestation", "fourniture", "travaux",
                    "sp√©cifications techniques", "crit√®res d'√©valuation", "d√©lai d'ex√©cution"
                ],
                "patterns": [
                    r"cahier\s+des?\s+charges?",
                    r"appel\s+d[''']offres?",
                    r"march√©\s+public",
                    r"consultation\s+publique"
                ]
            },
            "rapport": {
                "keywords": [
                    "rapport", "bilan", "√©valuation", "analyse", "√©tude", "diagnostic",
                    "recommandations", "conclusions", "synth√®se", "√©tat des lieux",
                    "performance", "r√©sultats", "indicateurs"
                ],
                "patterns": [
                    r"rapport\s+(annuel|mensuel|trimestriel|d[''']activit√©)",
                    r"bilan\s+d[''']activit√©",
                    r"√©valuation\s+des?\s+performances?"
                ]
            },
            "arrete_decret": {
                "keywords": [
                    "arr√™t√©", "d√©cret", "ministre", "secr√©taire d'√©tat", "dispositions",
                    "article", "vu le", "consid√©rant", "arr√™te", "d√©cr√®te",
                    "journal officiel", "publication", "abrog√©", "modifi√©"
                ],
                "patterns": [
                    r"arr√™t√©\s+(minist√©riel|n¬∞)",
                    r"d√©cret\s+(n¬∞|du)",
                    r"vu\s+le\s+d√©cret",
                    r"journal\s+officiel"
                ]
            },
            "circulaire": {
                "keywords": [
                    "circulaire", "instruction", "directive", "note de service",
                    "application", "mise en ≈ìuvre", "proc√©dure", "modalit√©s"
                ],
                "patterns": [
                    r"circulaire\s+n¬∞",
                    r"note\s+de\s+service",
                    r"instruction\s+minist√©rielle"
                ]
            },
            "contrat": {
                "keywords": [
                    "contrat", "convention", "accord", "partenariat", "engagement",
                    "parties", "obligations", "dur√©e", "r√©siliation", "clause"
                ],
                "patterns": [
                    r"contrat\s+de",
                    r"convention\s+de",
                    r"accord\s+de\s+partenariat"
                ]
            }
        }
        
        # Dictionnaires de classification - Arabe
        self.types_documents_ar = {
            "cahier_charges": {
                "keywords": [
                    "ÿØŸÅÿ™ÿ± ÿßŸÑÿ™ÿ≠ŸÖŸÑÿßÿ™", "ÿ∑ŸÑÿ® ÿπÿ±Ÿàÿ∂", "ÿµŸÅŸÇÿ© ÿπŸÖŸàŸÖŸäÿ©", "ÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿ©",
                    "ŸÖÿ™ÿ±ÿ¥ÿ≠", "ŸÖŸÇÿØŸÖ ÿßŸÑÿπÿ±ÿ∂", "ÿÆÿØŸÖÿ©", "ÿ™Ÿàÿ±ŸäÿØ", "ÿ£ÿ¥ÿ∫ÿßŸÑ",
                    "ÿßŸÑŸÖŸàÿßÿµŸÅÿßÿ™ ÿßŸÑÿ™ŸÇŸÜŸäÿ©", "ŸÖÿπÿßŸäŸäÿ± ÿßŸÑÿ™ŸÇŸäŸäŸÖ", "ÿ¢ÿ¨ÿßŸÑ ÿßŸÑÿ™ŸÜŸÅŸäÿ∞", "ÿßŸÑŸÖŸÜÿßŸÇÿµÿ©"
                ],
                "patterns": [
                    r"ÿØŸÅÿ™ÿ±\s+ÿßŸÑÿ™ÿ≠ŸÖŸÑÿßÿ™",
                    r"ÿ∑ŸÑÿ®\s+ÿπÿ±Ÿàÿ∂",
                    r"ÿµŸÅŸÇÿ©\s+ÿπŸÖŸàŸÖŸäÿ©",
                    r"ÿßÿ≥ÿ™ÿ¥ÿßÿ±ÿ©\s+ÿπŸÖŸàŸÖŸäÿ©"
                ]
            },
            "rapport": {
                "keywords": [
                    "ÿ™ŸÇÿ±Ÿäÿ±", "ÿ≠ÿµŸäŸÑÿ©", "ÿ™ŸÇŸäŸäŸÖ", "ÿ™ÿ≠ŸÑŸäŸÑ", "ÿØÿ±ÿßÿ≥ÿ©", "ÿ™ÿ¥ÿÆŸäÿµ",
                    "ÿ™ŸàÿµŸäÿßÿ™", "ÿÆŸÑÿßÿµÿßÿ™", "ŸÖŸÑÿÆÿµ", "Ÿàÿ∂ÿπŸäÿ©", "ÿ£ÿØÿßÿ°", "ŸÜÿ™ÿßÿ¶ÿ¨", "ŸÖÿ§ÿ¥ÿ±ÿßÿ™"
                ],
                "patterns": [
                    r"ÿ™ŸÇÿ±Ÿäÿ±\s+(ÿ≥ŸÜŸàŸä|ÿ¥Ÿáÿ±Ÿä|ŸÅÿµŸÑŸä|ÿßŸÑŸÜÿ¥ÿßÿ∑)",
                    r"ÿ≠ÿµŸäŸÑÿ©\s+ÿßŸÑŸÜÿ¥ÿßÿ∑",
                    r"ÿ™ŸÇŸäŸäŸÖ\s+ÿßŸÑÿ£ÿØÿßÿ°"
                ]
            },
            "arrete_decret": {
                "keywords": [
                    "ŸÇÿ±ÿßÿ±", "ŸÖÿ±ÿ≥ŸàŸÖ", "Ÿàÿ≤Ÿäÿ±", "ŸÉÿßÿ™ÿ® ÿßŸÑÿØŸàŸÑÿ©", "ŸÖŸÇÿ™ÿ∂Ÿäÿßÿ™",
                    "ŸÖÿßÿØÿ©", "ÿ®ŸÜÿßÿ° ÿπŸÑŸâ", "ÿßÿπÿ™ÿ®ÿßÿ±ÿß", "ŸäŸÇÿ±ÿ±", "Ÿäÿ±ÿ≥ŸÖ",
                    "ÿßŸÑÿ¨ÿ±ŸäÿØÿ© ÿßŸÑÿ±ÿ≥ŸÖŸäÿ©", "ŸÜÿ¥ÿ±", "ŸÖŸÑÿ∫Ÿâ", "ŸÖÿπÿØŸÑ"
                ],
                "patterns": [
                    r"ŸÇÿ±ÿßÿ±\s+(Ÿàÿ≤ÿßÿ±Ÿä|ÿ±ŸÇŸÖ)",
                    r"ŸÖÿ±ÿ≥ŸàŸÖ\s+(ÿ±ŸÇŸÖ|ÿ®ÿ™ÿßÿ±ŸäÿÆ)",
                    r"ÿ®ŸÜÿßÿ°\s+ÿπŸÑŸâ\s+ÿßŸÑŸÖÿ±ÿ≥ŸàŸÖ",
                    r"ÿßŸÑÿ¨ÿ±ŸäÿØÿ©\s+ÿßŸÑÿ±ÿ≥ŸÖŸäÿ©"
                ]
            },
            "circulaire": {
                "keywords": [
                    "ŸÖŸÜÿ¥Ÿàÿ±", "ÿ™ÿπŸÑŸäŸÖÿ©", "ÿ™Ÿàÿ¨ŸäŸá", "ŸÖÿ∞ŸÉÿ±ÿ© ÿÆÿØŸÖÿ©",
                    "ÿ™ÿ∑ÿ®ŸäŸÇ", "ÿ™ŸÅÿπŸäŸÑ", "ÿ•ÿ¨ÿ±ÿßÿ°", "ŸÉŸäŸÅŸäÿßÿ™"
                ],
                "patterns": [
                    r"ŸÖŸÜÿ¥Ÿàÿ±\s+ÿ±ŸÇŸÖ",
                    r"ŸÖÿ∞ŸÉÿ±ÿ©\s+ÿÆÿØŸÖÿ©",
                    r"ÿ™ÿπŸÑŸäŸÖÿ©\s+Ÿàÿ≤ÿßÿ±Ÿäÿ©"
                ]
            },
            "contrat": {
                "keywords": [
                    "ÿπŸÇÿØ", "ÿßÿ™ŸÅÿßŸÇŸäÿ©", "ÿßÿ™ŸÅÿßŸÇ", "ÿ¥ÿ±ÿßŸÉÿ©", "ÿßŸÑÿ™ÿ≤ÿßŸÖ",
                    "ÿ£ÿ∑ÿ±ÿßŸÅ", "ÿßŸÑÿ™ÿ≤ÿßŸÖÿßÿ™", "ŸÖÿØÿ©", "ŸÅÿ≥ÿÆ", "ÿ®ŸÜÿØ"
                ],
                "patterns": [
                    r"ÿπŸÇÿØ\s+",
                    r"ÿßÿ™ŸÅÿßŸÇŸäÿ©\s+",
                    r"ÿßÿ™ŸÅÿßŸÇ\s+ÿ¥ÿ±ÿßŸÉÿ©"
                ]
            }
        }
        
        # Secteurs - Fran√ßais et Arabe
        self.secteurs_fr = {
            "transports_routiers": [
                "transport routier", "autoroute", "route nationale", "circulation routi√®re",
                "permis de conduire", "v√©hicule", "camion", "bus", "autocar",
                "s√©curit√© routi√®re", "code de la route", "signalisation", "voiture", "NARSA"
            ],
            "logistique": [
                "logistique", "cha√Æne d'approvisionnement", "entreposage", "stockage",
                "distribution", "supply chain", "flux de marchandises", "plateforme logistique", "AMDL", "SNTL"
            ],
            "transport_aerien": [
                "transport a√©rien", "aviation", "a√©roport", "compagnie a√©rienne",
                "avion", "vol", "navigation a√©rienne", "contr√¥le a√©rien",
                "s√©curit√© a√©rienne", "aviation civile", "AIAC", "ONDA", "RAM"
            ],
            "marine_marchande": [
                "marine marchande", "transport maritime", "port", "navire", "bateau",
                "maritime", "shipping", "fret maritime", "armateur", "capitainerie", "ISEM"
            ],
            "transport_ferroviaire": [
                "transport ferroviaire", "chemin de fer", "train", "ONCF", "gare",
                "voie ferr√©e", "locomotive", "wagon", "rail", "infrastructure ferroviaire"
            ],
            "gouvernance": [
                "gouvernance", "politique publique", "administration", "minist√®re",
                "direction g√©n√©rale", "r√©forme", "strat√©gie", "pilotage", "coordination"
            ]
        }
        
        self.secteurs_ar = {
            "transports_routiers": [
                "ÿßŸÑŸÜŸÇŸÑ ÿßŸÑÿ∑ÿ±ŸÇŸä", "ÿßŸÑÿ∑ÿ±ŸäŸÇ ÿßŸÑÿ≥Ÿäÿßÿ±", "ÿßŸÑÿ∑ÿ±ŸäŸÇ ÿßŸÑŸàÿ∑ŸÜŸäÿ©", "ÿ≠ÿ±ŸÉÿ© ÿßŸÑŸÖÿ±Ÿàÿ±",
                "ÿ±ÿÆÿµÿ© ÿßŸÑÿ≥ŸäÿßŸÇÿ©", "ŸÖÿ±ŸÉÿ®ÿ©", "ÿ¥ÿßÿ≠ŸÜÿ©", "ÿ≠ÿßŸÅŸÑÿ©", "ÿ£Ÿàÿ™ŸàŸÉÿßÿ±",
                "ÿßŸÑÿ≥ŸÑÿßŸÖÿ© ÿßŸÑÿ∑ÿ±ŸÇŸäÿ©", "ŸÖÿØŸàŸÜÿ© ÿßŸÑÿ≥Ÿäÿ±", "ÿßŸÑÿ•ÿ¥ÿßÿ±ÿßÿ™ ÿßŸÑŸÖÿ±Ÿàÿ±Ÿäÿ©"
            ],
            "logistique": [
                "ÿßŸÑŸÑŸàÿ¨ÿ≥ÿ™ŸäŸÉ", "ÿ≥ŸÑÿ≥ŸÑÿ© ÿßŸÑÿ™ŸÖŸàŸäŸÜ", "ÿßŸÑÿ™ÿÆÿ≤ŸäŸÜ", "ÿßŸÑŸÖÿ≥ÿ™ŸàÿØÿπÿßÿ™",
                "ÿßŸÑÿ™Ÿàÿ≤Ÿäÿπ", "ÿ™ÿØŸÅŸÇ ÿßŸÑÿ®ÿ∂ÿßÿ¶ÿπ", "ÿßŸÑŸÖŸÜÿµÿ© ÿßŸÑŸÑŸàÿ¨ÿ≥ÿ™ŸäŸÉŸäÿ©"
            ],
            "transport_aerien": [
                "ÿßŸÑŸÜŸÇŸÑ ÿßŸÑÿ¨ŸàŸä", "ÿßŸÑÿ∑Ÿäÿ±ÿßŸÜ", "ŸÖÿ∑ÿßÿ±", "ÿ¥ÿ±ŸÉÿ© ÿ∑Ÿäÿ±ÿßŸÜ",
                "ÿ∑ÿßÿ¶ÿ±ÿ©", "ÿ±ÿ≠ŸÑÿ©", "ÿßŸÑŸÖŸÑÿßÿ≠ÿ© ÿßŸÑÿ¨ŸàŸäÿ©", "ÿßŸÑŸÖÿ±ÿßŸÇÿ®ÿ© ÿßŸÑÿ¨ŸàŸäÿ©",
                "ÿßŸÑÿ≥ŸÑÿßŸÖÿ© ÿßŸÑÿ¨ŸàŸäÿ©", "ÿßŸÑÿ∑Ÿäÿ±ÿßŸÜ ÿßŸÑŸÖÿØŸÜŸä"
            ],
            "marine_marchande": [
                "ÿßŸÑÿ£ÿ≥ÿ∑ŸàŸÑ ÿßŸÑÿ™ÿ¨ÿßÿ±Ÿä", "ÿßŸÑŸÜŸÇŸÑ ÿßŸÑÿ®ÿ≠ÿ±Ÿä", "ŸÖŸäŸÜÿßÿ°", "ÿ≥ŸÅŸäŸÜÿ©", "ÿ®ÿßÿÆÿ±ÿ©",
                "ÿ®ÿ≠ÿ±Ÿä", "ÿßŸÑÿ¥ÿ≠ŸÜ ÿßŸÑÿ®ÿ≠ÿ±Ÿä", "ŸÖÿßŸÑŸÉ ÿßŸÑÿ≥ŸÅŸäŸÜÿ©", "ŸÇÿ®ÿ∑ÿßŸÜŸäÿ© ÿßŸÑŸÖŸäŸÜÿßÿ°"
            ],
            "transport_ferroviaire": [
                "ÿßŸÑŸÜŸÇŸÑ ÿ®ÿßŸÑÿ≥ŸÉŸÉ ÿßŸÑÿ≠ÿØŸäÿØŸäÿ©", "ÿßŸÑÿ≥ŸÉÿ© ÿßŸÑÿ≠ÿØŸäÿØŸäÿ©", "ŸÇÿ∑ÿßÿ±", "ÿßŸÑŸÖŸÉÿ™ÿ® ÿßŸÑŸàÿ∑ŸÜŸä ŸÑŸÑÿ≥ŸÉŸÉ ÿßŸÑÿ≠ÿØŸäÿØŸäÿ©",
                "ŸÖÿ≠ÿ∑ÿ©", "ÿÆÿ∑ ÿßŸÑÿ≥ŸÉÿ© ÿßŸÑÿ≠ÿØŸäÿØŸäÿ©", "ŸÇÿßÿ∑ÿ±ÿ©", "ÿπÿ±ÿ®ÿ©", "ÿ≥ŸÉÿ©", "ÿßŸÑÿ®ŸÜŸäÿ© ÿßŸÑÿ™ÿ≠ÿ™Ÿäÿ© ŸÑŸÑÿ≥ŸÉŸÉ ÿßŸÑÿ≠ÿØŸäÿØŸäÿ©"
            ],
            "gouvernance": [
                "ÿßŸÑÿ≠ŸÉÿßŸÖÿ©", "ÿßŸÑÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿπŸÖŸàŸÖŸäÿ©", "ÿßŸÑÿ•ÿØÿßÿ±ÿ©", "Ÿàÿ≤ÿßÿ±ÿ©",
                "ÿßŸÑŸÖÿØŸäÿ±Ÿäÿ© ÿßŸÑÿπÿßŸÖÿ©", "ÿ•ÿµŸÑÿßÿ≠", "ÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ©", "ŸÇŸäÿßÿØÿ©", "ÿ™ŸÜÿ≥ŸäŸÇ"
            ]
        }
        
        # Patterns pour les entit√©s - Fran√ßais et Arabe
        self.patterns_fr = {
            "dates": [
                r"\b\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{4}\b",
                r"\b\d{1,2}\s+(janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}\b",
                r"\b(janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}\b",
                r"\b\d{4}\b"
            ],
            "references": [
                r"\bn¬∞\s*\d+[\-\/]\d+",
                r"\b(arr√™t√©|d√©cret|circulaire)\s+n¬∞\s*[\d\-\/A-Z]+",
                r"\br√©f√©rence\s*:\s*[\w\-\/]+",
                r"\b[A-Z]{2,}\s*\d+[\-\/]\d+"
            ]
        }
        
        self.patterns_ar = {
            "dates": [
                r"\b\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{4}\b",  # Format num√©rique
                r"\b\d{1,2}\s+(ŸäŸÜÿßŸäÿ±|ŸÅÿ®ÿ±ÿßŸäÿ±|ŸÖÿßÿ±ÿ≥|ÿ£ÿ®ÿ±ŸäŸÑ|ŸÖÿßŸä|ŸäŸàŸÜŸäŸà|ŸäŸàŸÑŸäŸàÿ≤|ÿ∫ÿ¥ÿ™|ÿ¥ÿ™ŸÜÿ®ÿ±|ÿ£ŸÉÿ™Ÿàÿ®ÿ±|ŸÜŸàŸÜÿ®ÿ±|ÿØÿ¨ŸÜÿ®ÿ±)\s+\d{4}\b",
                r"\b(ŸäŸÜÿßŸäÿ±|ŸÅÿ®ÿ±ÿßŸäÿ±|ŸÖÿßÿ±ÿ≥|ÿ£ÿ®ÿ±ŸäŸÑ|ŸÖÿßŸä|ŸäŸàŸÜŸäŸà|ŸäŸàŸÑŸäŸàÿ≤|ÿ∫ÿ¥ÿ™|ÿ¥ÿ™ŸÜÿ®ÿ±|ÿ£ŸÉÿ™Ÿàÿ®ÿ±|ŸÜŸàŸÜÿ®ÿ±|ÿØÿ¨ŸÜÿ®ÿ±)\s+\d{4}\b",
                r"\bÿ≥ŸÜÿ©\s+\d{4}\b",
                r"\b\d{4}\s+ŸáŸÄ\b"  # Ann√©e h√©girienne
            ],
            "references": [
                r"\bÿ±ŸÇŸÖ\s*\d+[\-\/]\d+",
                r"\b(ŸÇÿ±ÿßÿ±|ŸÖÿ±ÿ≥ŸàŸÖ|ŸÖŸÜÿ¥Ÿàÿ±)\s+ÿ±ŸÇŸÖ\s*[\d\-\/ÿ£-Ÿä]+",
                r"\bŸÖÿ±ÿ¨ÿπ\s*:\s*[\w\-\/]+",
                r"\b[ÿ£-Ÿä]{2,}\s*\d+[\-\/]\d+"
            ]
        }
        
        # Lieux du Maroc en fran√ßais et arabe
        self.lieux_maroc = {
            "fr": [
                r"\b(Rabat|Casablanca|Marrakech|F√®s|Tanger|Agadir|Mekn√®s|Oujda|Kenitra|T√©touan|Sal√©)\b",
                r"\b(r√©gion|province|pr√©fecture)\s+de\s+\w+",
                r"\ba√©roport\s+(Mohammed V|de Rabat-Sal√©|de Marrakech)",
                r"\bport\s+de\s+(Casablanca|Tanger|Agadir)"
            ],
            "ar": [
                r"\b(ÿßŸÑÿ±ÿ®ÿßÿ∑|ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ°|ŸÖÿ±ÿßŸÉÿ¥|ŸÅÿßÿ≥|ÿ∑ŸÜÿ¨ÿ©|ÿ£ŸÉÿßÿØŸäÿ±|ŸÖŸÉŸÜÿßÿ≥|Ÿàÿ¨ÿØÿ©|ÿßŸÑŸÇŸÜŸäÿ∑ÿ±ÿ©|ÿ™ÿ∑ŸàÿßŸÜ|ÿ≥ŸÑÿß)\b",
                r"\b(ÿ¨Ÿáÿ©|ÿ•ŸÇŸÑŸäŸÖ|ÿπŸÖÿßŸÑÿ©)\s+\w+",
                r"\bmÿ∑ÿßÿ±\s+(ŸÖÿ≠ŸÖÿØ ÿßŸÑÿÆÿßŸÖÿ≥|ÿßŸÑÿ±ÿ®ÿßÿ∑ ÿ≥ŸÑÿß|ŸÖÿ±ÿßŸÉÿ¥)",
                r"\bŸÖŸäŸÜÿßÿ°\s+(ÿßŸÑÿØÿßÿ± ÿßŸÑÿ®Ÿäÿ∂ÿßÿ°|ÿ∑ŸÜÿ¨ÿ©|ÿ£ŸÉÿßÿØŸäÿ±)"
            ]
        }
    
    def detect_language(self, text: str) -> str:
        """D√©tection de la langue du document"""
        # Comptage des caract√®res arabes
        arabic_chars = len(re.findall(r'[\u0600-\u06FF]', text))
        # Comptage des caract√®res latins
        latin_chars = len(re.findall(r'[a-zA-Z√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ß√Ä√Ç√Ñ√â√à√ä√ã√è√é√î√ñ√ô√õ√ú√á]', text))
        
        total_chars = arabic_chars + latin_chars
        
        if total_chars == 0:
            return "unknown"
        
        arabic_ratio = arabic_chars / total_chars
        
        if arabic_ratio > 0.6:
            return "ar"
        elif arabic_ratio < 0.2:
            return "fr"
        else:
            return "mixed"
    
    def extract_entities_with_spacy(self, text: str, language: str) -> Tuple[List[str], List[str], List[str]]:
        """Extraction d'entit√©s avec spaCy (pour le fran√ßais principalement)"""
        personnes, lieux, organisations = [], [], []
        
        if language in ["fr", "mixed"] and self.nlp_fr:
            doc = self.nlp_fr(text)
            for ent in doc.ents:
                if ent.label_ == "PER":
                    personnes.append(ent.text.strip())
                elif ent.label_ in ["LOC", "GPE"]:
                    lieux.append(ent.text.strip())
                elif ent.label_ == "ORG":
                    organisations.append(ent.text.strip())
        
        # Pour l'arabe, utilisation de patterns manuels (en l'absence de spaCy arabe)
        if language in ["ar", "mixed"]:
            # Extraction des noms propres arabes (patterns simples)
            arabic_names = re.findall(r'\b[ÿ£-Ÿä]{3,}\s+[ÿ£-Ÿä]{3,}\b', text)
            personnes.extend(arabic_names)
            
            # Extraction des organisations gouvernementales arabes
            org_patterns = [
                r'Ÿàÿ≤ÿßÿ±ÿ©\s+[ÿ£-Ÿä\s]+',
                r'ÿßŸÑŸÖÿØŸäÿ±Ÿäÿ©\s+ÿßŸÑÿπÿßŸÖÿ©\s+[ÿ£-Ÿä\s]+',
                r'ÿßŸÑŸÖŸÉÿ™ÿ®\s+ÿßŸÑŸàÿ∑ŸÜŸä\s+[ÿ£-Ÿä\s]+'
            ]
            for pattern in org_patterns:
                matches = re.findall(pattern, text)
                organisations.extend(matches)
        
        return personnes, lieux, organisations
    
    def extract_dates(self, text: str, language: str) -> List[str]:
        """Extraction des dates selon la langue"""
        dates = []
        
        if language in ["fr", "mixed"]:
            for pattern in self.patterns_fr["dates"]:
                matches = re.findall(pattern, text, re.IGNORECASE)
                dates.extend(matches)
        
        if language in ["ar", "mixed"]:
            for pattern in self.patterns_ar["dates"]:
                matches = re.findall(pattern, text)
                dates.extend(matches)
        
        return list(set([date.strip() for date in dates if len(date.strip()) > 3]))
    
    def extract_references(self, text: str, language: str) -> List[str]:
        """Extraction des num√©ros de r√©f√©rence selon la langue"""
        references = []
        
        if language in ["fr", "mixed"]:
            for pattern in self.patterns_fr["references"]:
                matches = re.findall(pattern, text, re.IGNORECASE)
                references.extend(matches)
        
        if language in ["ar", "mixed"]:
            for pattern in self.patterns_ar["references"]:
                matches = re.findall(pattern, text)
                references.extend(matches)
        
        return list(set(references))
    
    def extract_moroccan_locations(self, text: str, language: str) -> List[str]:
        """Extraction des lieux marocains selon la langue"""
        lieux = []
        
        if language in ["fr", "mixed"]:
            for pattern in self.lieux_maroc["fr"]:
                matches = re.findall(pattern, text, re.IGNORECASE)
                lieux.extend(matches)
        
        if language in ["ar", "mixed"]:
            for pattern in self.lieux_maroc["ar"]:
                matches = re.findall(pattern, text)
                lieux.extend(matches)
        
        return list(set(lieux))
    
    def classify_document_type(self, text: str, language: str) -> Tuple[str, float]:
        """Classification du type de document selon la langue"""
        text_lower = text.lower()
        scores = {}
        
        # S√©lection du dictionnaire selon la langue
        if language == "ar":
            types_dict = self.types_documents_ar
        elif language == "mixed":
            # Fusion des deux dictionnaires pour documents mixtes
            types_dict = {}
            for doc_type in self.types_documents_fr:
                types_dict[doc_type] = {
                    "keywords": self.types_documents_fr[doc_type]["keywords"] + self.types_documents_ar[doc_type]["keywords"],
                    "patterns": self.types_documents_fr[doc_type]["patterns"] + self.types_documents_ar[doc_type]["patterns"]
                }
        else:  # fran√ßais
            types_dict = self.types_documents_fr
        
        for doc_type, config in types_dict.items():
            score = 0
            
            # Score bas√© sur les mots-cl√©s
            for keyword in config["keywords"]:
                if language == "ar" or language == "mixed":
                    count = text.count(keyword)  # Pas de lower() pour l'arabe
                else:
                    count = text_lower.count(keyword.lower())
                score += count * 2
            
            # Score bas√© sur les patterns regex
            for pattern in config["patterns"]:
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                score += matches * 3
            
            scores[doc_type] = score
        
        if scores:
            best_type = max(scores, key=scores.get)
            max_score = scores[best_type]
            
            total_score = sum(scores.values())
            confidence = (max_score / total_score * 100) if total_score > 0 else 0
            
            return best_type, min(confidence, 100.0)
        
        return "document_general", 0.0
    
    def identify_sector(self, text: str, language: str) -> str:
        """Identification du secteur selon la langue"""
        text_lower = text.lower()
        sector_scores = {}
        
        # S√©lection du dictionnaire des secteurs
        if language == "ar":
            secteurs_dict = self.secteurs_ar
        elif language == "mixed":
            secteurs_dict = {}
            for sector in self.secteurs_fr:
                secteurs_dict[sector] = self.secteurs_fr[sector] + self.secteurs_ar[sector]
        else:
            secteurs_dict = self.secteurs_fr
        
        for sector, keywords in secteurs_dict.items():
            score = 0
            for keyword in keywords:
                if language == "ar" or language == "mixed":
                    count = text.count(keyword)
                else:
                    count = text_lower.count(keyword.lower())
                score += count
            sector_scores[sector] = score
        
        if sector_scores:
            best_sector = max(sector_scores, key=sector_scores.get)
            if sector_scores[best_sector] > 0:
                return best_sector
        
        return "gouvernance"
    
    def extract_keywords(self, text: str, language: str, top_n: int = 10) -> List[str]:
        """Extraction des mots-cl√©s selon la langue"""
        if language == "ar" or language == "mixed":
            # Pour l'arabe : extraction des mots de plus de 3 caract√®res arabes
            words = re.findall(r'[\u0600-\u06FF]{3,}', text)
            
            # Mots vides arabes basiques
            arabic_stop_words = {
                'ŸÅŸä', 'ŸÖŸÜ', 'ÿ•ŸÑŸâ', 'ÿπŸÑŸâ', 'ÿπŸÜ', 'ŸÖÿπ', 'ŸÉŸÑ', 'Ÿáÿ∞ÿß', 'Ÿáÿ∞Ÿá',
                'ÿ∞ŸÑŸÉ', 'ÿßŸÑÿ™Ÿä', 'ÿßŸÑÿ∞Ÿä', 'ŸàŸáŸä', 'ŸàŸáŸà', 'ŸÉŸÖÿß', 'ŸÑŸÉŸÜ', 'ÿ£ŸÜ'
            }
            words = [word for word in words if word not in arabic_stop_words]
        
        if language == "fr" or language == "mixed":
            # Pour le fran√ßais
            stop_words_fr = {
                'le', 'de', 'un', '√†', '√™tre', 'et', 'en', 'avoir', 'que', 'pour',
                'dans', 'ce', 'il', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout',
                'plus', 'par', 'grand', 'mais', 'si', 'ou', 'son', 'du', 'les', 'des'
            }
            
            fr_words = re.findall(r'\b[a-zA-Z√†√¢√§√©√®√™√´√Ø√Æ√¥√∂√π√ª√º√ß]{3,}\b', text.lower())
            fr_words = [word for word in fr_words if word not in stop_words_fr]
            
            if language == "mixed":
                words.extend(fr_words)
            else:
                words = fr_words
        
        # Comptage et s√©lection des plus fr√©quents
        word_counts = Counter(words)
        return [word for word, count in word_counts.most_common(top_n)]
    
    def analyze_document(self, text: str) -> DocumentAnalysis:
        """Analyse compl√®te d'un document bilingue"""
        logger.info("D√©but de l'analyse du document...")
        
        # D√©tection de la langue
        language = self.detect_language(text)
        logger.info(f"Langue d√©tect√©e: {language}")
        
        # Classification du type de document
        doc_type, confidence = self.classify_document_type(text, language)
        logger.info(f"Type de document identifi√©: {doc_type} (confiance: {confidence:.1f}%)")
        
        # Extraction des entit√©s
        personnes_spacy, lieux_spacy, organisations = self.extract_entities_with_spacy(text, language)
        
        # Extraction des lieux sp√©cifiques au Maroc
        lieux_maroc = self.extract_moroccan_locations(text, language)
        
        # Combinaison des lieux
        lieux = list(set(lieux_spacy + lieux_maroc))
        
        # Autres extractions
        dates = self.extract_dates(text, language)
        references = self.extract_references(text, language)
        secteur = self.identify_sector(text, language)
        mots_cles = self.extract_keywords(text, language)
        
        # Cr√©ation du r√©sultat structur√©
        analysis = DocumentAnalysis(
            type_document=doc_type,
            confidence_score=confidence,
            language=language,
            personnes=list(set(personnes_spacy)),
            lieux=lieux,
            dates=dates,
            secteur=secteur,
            mots_cles=mots_cles,
            organisations=list(set(organisations)),
            numeros_reference=references
        )
        
        logger.info("Analyse termin√©e avec succ√®s")
        return analysis
    
    def export_analysis_json(self, analysis: DocumentAnalysis, filename: str = None) -> str:
        """Export de l'analyse en format JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"analyse_document_{timestamp}.json"
        
        analysis_dict = asdict(analysis)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis_dict, f, ensure_ascii=False, indent=2)
        
        return filename
    
    def print_analysis_report(self, analysis: DocumentAnalysis):
        """Affichage d'un rapport d'analyse format√©"""
        print("="*60)
        print("üìÑ RAPPORT D'ANALYSE DOCUMENTAIRE BILINGUE")
        print("="*60)
        
        # Mapping des langues pour l'affichage
        lang_display = {"fr": "Fran√ßais", "ar": "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©", "mixed": "Mixte (Fr/Ar)", "unknown": "Inconnue"}
        
        print(f"\nüåê Langue: {lang_display.get(analysis.language, analysis.language)}")
        print(f"üè∑Ô∏è  Type de document: {analysis.type_document.replace('_', ' ').title()}")
        print(f"üìä Confiance: {analysis.confidence_score:.1f}%")
        print(f"üè¢ Secteur: {analysis.secteur.replace('_', ' ').title()}")
        
        print(f"\nüë• Personnes identifi√©es ({len(analysis.personnes)}):")
        for personne in analysis.personnes[:5]:
            print(f"   ‚Ä¢ {personne}")
        
        print(f"\nüìç Lieux mentionn√©s ({len(analysis.lieux)}):")
        for lieu in analysis.lieux[:5]:
            print(f"   ‚Ä¢ {lieu}")
        
        print(f"\nüìÖ Dates trouv√©es ({len(analysis.dates)}):")
        for date in analysis.dates[:5]:
            print(f"   ‚Ä¢ {date}")
        
        print(f"\nüèõÔ∏è  Organisations ({len(analysis.organisations)}):")
        for org in analysis.organisations[:5]:
            print(f"   ‚Ä¢ {org}")
        
        print(f"\nüî¢ R√©f√©rences ({len(analysis.numeros_reference)}):")
        for ref in analysis.numeros_reference[:3]:
            print(f"   ‚Ä¢ {ref}")
        
        print(f"\nüîë Mots-cl√©s principaux:")
        for mot in analysis.mots_cles[:8]:
            print(f"   ‚Ä¢ {mot}")
        
        print("\n" + "="*60)


# Fonction utilitaire pour l'int√©gration dans votre pipeline
def integrate_with_extractors(text_extrait: str, analyzer: BilingualDocumentAnalyzer = None) -> Dict:
    """
    Fonction d'int√©gration avec vos extractors existants
    
    Args:
        text_extrait: Texte extrait par vos extractors
        analyzer: Instance de l'analyseur (optionnel)
    
    Returns:
        Dict: R√©sultats de l'analyse au format dictionnaire
    """
    if analyzer is None:
        analyzer = BilingualDocumentAnalyzer()
    
    # Analyse du document
    analysis = analyzer.analyze_document(text_extrait)
    
    # Conversion en dictionnaire pour faciliter l'int√©gration
    result = asdict(analysis)
    
    # Ajout de m√©tadonn√©es utiles
    result['metadata'] = {
        'analysis_timestamp': datetime.now().isoformat(),
        'text_length': len(text_extrait),
        'entities_total': (len(analysis.personnes) + len(analysis.lieux) + 
                          len(analysis.organisations) + len(analysis.dates)),
        'processing_success': True
    }
    
    return result

# Classe utilitaire pour le batch processing
class BatchDocumentProcessor:
    """Traitement par lot de documents"""
    
    def __init__(self):
        self.analyzer = BilingualDocumentAnalyzer()
        self.results = []
    
    def process_documents(self, documents: List[Dict[str, str]]) -> List[Dict]:
        """
        Traite une liste de documents
        
        Args:
            documents: Liste de dictionnaires avec 'id', 'text', 'filename' (optionnel)
        
        Returns:
            Liste des analyses
        """
        results = []
        
        for i, doc in enumerate(documents):
            try:
                logger.info(f"Traitement du document {i+1}/{len(documents)}")
                
                analysis = self.analyzer.analyze_document(doc['text'])
                result = asdict(analysis)
                result['document_id'] = doc.get('id', f'doc_{i+1}')
                result['filename'] = doc.get('filename', f'document_{i+1}.txt')
                result['processing_status'] = 'success'
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Erreur lors du traitement du document {i+1}: {e}")
                results.append({
                    'document_id': doc.get('id', f'doc_{i+1}'),
                    'filename': doc.get('filename', f'document_{i+1}.txt'),
                    'processing_status': 'error',
                    'error_message': str(e)
                })
        
        return results
    
    def export_batch_results(self, results: List[Dict], output_file: str = None) -> str:
        """Export des r√©sultats de traitement par lot"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"batch_analysis_{timestamp}.json"
        
        # Calcul des statistiques
        total_docs = len(results)
        successful = len([r for r in results if r.get('processing_status') == 'success'])
        failed = total_docs - successful
        
        batch_summary = {
            'batch_info': {
                'total_documents': total_docs,
                'successful_analyses': successful,
                'failed_analyses': failed,
                'success_rate': (successful / total_docs * 100) if total_docs > 0 else 0,
                'processing_timestamp': datetime.now().isoformat()
            },
            'document_analyses': results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(batch_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"R√©sultats du traitement par lot export√©s vers: {output_file}")
        return output_file

    